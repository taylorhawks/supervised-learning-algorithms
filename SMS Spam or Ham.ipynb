# for interacting with .csv files
import csv
# natural language toolkit
import nltk
# utility functions that introduce randomness
import random
# for measuring time-related things
import time


def get_length_bucket(sms_length):
    if sms_length < 20:
        return "short"
    elif sms_length < 70:
        return "medium"
    else:
        return "long"

#message features

def sms_features(sms):
    return {
        "length": get_length_bucket(sms),
        "no cost": "free" in sms,
        "currency": "$" in sms or "£" in sms or "₤" in sms,
        "participation": "quiz" in sms or "contest" in sms,
        "internet": "download" in sms or ".com" in sms
}

#run get_feature_sets() over each sms message and add a row with the value for each feature

def get_feature_sets():
    f = open('/home/vagrant/repos/datasets/sms_spam_or_ham.csv', 'rb')

    # let's read in the rows from the csv file
    rows = []
    for row in csv.reader(f):
        rows.append(row)

    # now let's generate the output that we specified in the comments above
    output_data = []

    # let's just run it on 100,000 rows first, instead of all 1.5 million rows
    # when you experiment with the `twitter_features` function to improve accuracy
    # feel free to get rid of the row limit and just run it on the whole set
    for row in rows[:100000]:
        # Remember that row[0] is the label, either 0 or 1
        # and row[1] is the tweet body

        # get the label
        label = row[0]

        # get the tweet body and compute the feature dictionary
        feature_dict = sms_features(row[1])

        # add the tuple of feature_dict, label to output_data
        data = (feature_dict, label)
        output_data.append(data)

    # close the file
    f.close()
    return output_data

#use the first 20% of a segment of the data to determine the use of features for determinaton, then apply it to the next 80%

def get_training_and_validation_sets(feature_sets):
    random.shuffle(feature_sets)
    count = len(feature_sets)
    slicing_point = int(.20 * count)
    training_set = feature_sets[:slicing_point]
    validation_set = feature_sets[slicing_point:]
    return training_set, validation_set
def run_classification(training_set, validation_set):
    classifier = nltk.NaiveBayesClassifier.train(training_set)
    # let's see how accurate it was
    accuracy = nltk.classify.accuracy(classifier, validation_set)
    print "The accuracy was.... {}".format(accuracy)
    return classifier


#predicts the label of each sms message, ham or spam

def predict(classifier, new_sms):
    """
    Given a trained classifier and a fresh data point (a tweet),
    this will predict its label, either 0 or 1.
    """
    return classifier.classify(sms_features(new_sms))


# Now let's use the above functions to run our program
start_time = time.time()

print "SMS Messages: Spam or Ham?"

our_feature_sets = get_feature_sets()
our_training_set, our_validation_set = get_training_and_validation_sets(our_feature_sets)
print "Size of our data set: {}".format(len(our_feature_sets))

print "Now training the classifier and testing the accuracy..."
classifier = run_classification(our_training_set, our_validation_set)

end_time = time.time()
completion_time = end_time - start_time
print "It took {} seconds to run the algorithm".format(completion_time)
